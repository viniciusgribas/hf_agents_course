{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L_jWf74Nwa9"
      },
      "source": [
        "# Agentic RAG: turbocharge your RAG with query reformulation and self-query! ğŸš€\n",
        "_Authored by: [Aymeric Roucher](https://huggingface.co/m-ric)_\n",
        "\n",
        "> This tutorial is advanced. You should have notions from [this other cookbook](advanced_rag) first!\n",
        "\n",
        "> Reminder: Retrieval-Augmented-Generation (RAG) is â€œusing an LLM to answer a user query, but basing the answer on information retrieved from a knowledge baseâ€. It has many advantages over using a vanilla or fine-tuned LLM: to name a few, it allows to ground the answer on true facts and reduce confabulations, it allows to provide the LLM with domain-specific knowledge, and it allows fine-grained control of access to information from the knowledge base.\n",
        "\n",
        "But vanilla RAG has limitations, most importantly these two:\n",
        "- It **performs only one retrieval step**: if the results are bad, the generation in turn will be bad.\n",
        "- __Semantic similarity is computed with the *user query* as a reference__, which might be suboptimal: for instance, the user query will often be a question and the document containing the true answer will be in affirmative voice, so its similarity score will be downgraded compared to other source documents in the interrogative form, leading to a risk of missing the relevant information.\n",
        "\n",
        "But we can alleviate these problems by making a **RAG agent: very simply, an agent armed with a retriever tool!**\n",
        "\n",
        "This agent will: âœ… Formulate the query itself and âœ… Critique to re-retrieve if needed.\n",
        "\n",
        "So it should naively recover some advanced RAG techniques!\n",
        "- Instead of directly using the user query as the reference in semantic search, the agent formulates itself a reference sentence that can be closer to the targeted documents, as in [HyDE](https://huggingface.co/papers/2212.10496)\n",
        "- The agent can the generated snippets and re-retrieve if needed, as in [Self-Query](https://docs.llamaindex.ai/en/stable/examples/evaluation/RetryQuery/)\n",
        "\n",
        "Let's build this system. ğŸ› ï¸\n",
        "\n",
        "Run the line below to install required dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tLFlQRQpNwbF"
      },
      "outputs": [],
      "source": [
        "!pip install pandas langchain langchain-community sentence-transformers faiss-cpu smolagents --upgrade -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQXm1L2GNwbH"
      },
      "source": [
        "Let's login in order to call the HF Inference API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mV72H1JGNwbI"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a562abef4fb04a64ae76876ded813d70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWaEeoVHNwbI"
      },
      "source": [
        "We first load a knowledge base on which we want to perform RAG: this dataset is a compilation of the documentation pages for many `huggingface` packages, stored as markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EI9wxPjVNwbJ"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2647"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(knowledge_base)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMFQQOAKNwbK"
      },
      "source": [
        "Now we prepare the knowledge base by processing the dataset and storing it into a vector database to be used by the retriever.\n",
        "\n",
        "We use [LangChain](https://python.langchain.com/) for its excellent vector database utilities.\n",
        "For the embedding model, we use [thenlper/gte-small](https://huggingface.co/thenlper/gte-small) since it performed well in our `RAG_evaluation` cookbook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LHwM6Vj1NwbK",
        "outputId": "71509a3f-01c7-4a93-9e60-d54e977d31d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Splitting documents...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2647/2647 [01:11<00:00, 37.11it/s]\n",
            "/tmp/ipykernel_4376/2798339493.py:37: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding documents... This should take a few minutes (5 minutes on MacBook with M1 Pro)\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "\n",
        "source_docs = [\n",
        "    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]})\n",
        "    for doc in knowledge_base\n",
        "]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "    AutoTokenizer.from_pretrained(\"thenlper/gte-small\"),\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=20,\n",
        "    add_start_index=True,\n",
        "    strip_whitespace=True,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        ")\n",
        "\n",
        "# Split docs and keep only unique ones\n",
        "print(\"Splitting documents...\")\n",
        "docs_processed = []\n",
        "unique_texts = {}\n",
        "for doc in tqdm(source_docs):\n",
        "    new_docs = text_splitter.split_documents([doc])\n",
        "    for new_doc in new_docs:\n",
        "        if new_doc.page_content not in unique_texts:\n",
        "            unique_texts[new_doc.page_content] = True\n",
        "            docs_processed.append(new_doc)\n",
        "\n",
        "print(\n",
        "    \"Embedding documents... This should take a few minutes (5 minutes on MacBook with M1 Pro)\"\n",
        ")\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n",
        "vectordb = FAISS.from_documents(\n",
        "    documents=docs_processed,\n",
        "    embedding=embedding_model,\n",
        "    distance_strategy=DistanceStrategy.COSINE,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYIRyNz2NwbM"
      },
      "source": [
        "Now the database is ready: letâ€™s build our agentic RAG system!\n",
        "\n",
        "ğŸ‘‰ We only need a `RetrieverTool` that our agent can leverage to retrieve information from the knowledge base.\n",
        "\n",
        "Since we need to add a vectordb as an attribute of the tool, we cannot simply use the [simple tool constructor](https://huggingface.co/docs/transformers/main/en/agents#create-a-new-tool) with a `@tool` decorator: so we will follow the advanced setup highlighted in the [advanced agents documentation](https://huggingface.co/docs/transformers/main/en/agents_advanced#directly-define-a-tool-by-subclassing-tool-and-share-it-to-the-hub)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='55fd059f-1ae0-4920-9a6c-6802ad0d6057', metadata={'source': 'peft', 'start_index': 6133}, page_content='[Multitask prompt tuning (MPT)](https://hf.co/papers/2103.10385) learns a single prompt from data for multiple task types that can be shared for different target tasks. Other existing approaches learn a separate soft prompt for each task that need to be retrieved or aggregated for adaptation to target tasks. MPT consists of two stages:'),\n",
              " Document(id='14cbd31f-8018-4693-8195-0892fcc40e81', metadata={'source': 'optimum', 'start_index': 3884}, page_content=\"--task TASK           The task to export the model for. If not specified, the task will be auto-inferred based on the model. Available tasks depend on the model, but are among: ['default', 'fill-mask', 'text-generation', 'text2text-generation', 'text-classification', 'token-classification', 'multiple-choice', 'object-detection', 'question-answering', 'image-classification', 'image-segmentation', 'masked-im', 'semantic-segmentation', 'automatic-speech-recognition', 'audio-classification', 'audio-frame-classification', 'automatic-speech-recognition', 'audio-xvector', 'image-to-text', 'stable-diffusion', 'zero-shot-object-detection']\"),\n",
              " Document(id='be81ce4b-9fd5-402f-a960-2b464a61554b', metadata={'source': 'peft', 'start_index': 6472}, page_content='1. source training - for each task, its soft prompt is decomposed into task-specific vectors. The task-specific vectors are multiplied together to form another matrix W, and the Hadamard product is used between W and a shared prompt matrix P to generate a task-specific prompt matrix. The task-specific prompts are distilled into a single prompt matrix that is shared across all tasks. This prompt is trained with multitask training.\\n2. target adaptation - to adapt the single prompt for a target task, a target prompt is initialized and expressed as the Hadamard product of the shared prompt matrix and the task-specific low-rank prompt matrix.'),\n",
              " Document(id='d5e41f60-dd65-405c-96e3-8c9ac4d5bab1', metadata={'source': 'deep-rl-class', 'start_index': 3185}, page_content='### Q4: A task is an instance of a Reinforcement Learning problem. What are the two types of tasks?'),\n",
              " Document(id='459dde7a-d8ad-41f8-9658-515cef9be4a0', metadata={'source': 'optimum', 'start_index': 2291}, page_content=\"Optional arguments:\\n  --task TASK           The task to export the model for. If not specified, the task will be auto-inferred based on the model. Available tasks depend on\\n                        the model, but are among: ['default', 'fill-mask', 'text-generation', 'text2text-generation', 'text-classification', 'token-classification',\\n                        'multiple-choice', 'object-detection', 'question-answering', 'image-classification', 'image-segmentation', 'masked-im', 'semantic-\\n                        segmentation', 'automatic-speech-recognition', 'audio-classification', 'audio-frame-classification', 'automatic-speech-recognition', 'audio-xvector', 'vision2seq-\"),\n",
              " Document(id='50640039-daad-4f04-b42b-38946055ecad', metadata={'source': 'deep-rl-class', 'start_index': 1}, page_content='Type of tasks [[tasks]]\\n\\nA task is an **instance** of a Reinforcement Learning problem. We can have two types of tasks: **episodic** and **continuing**.\\n\\n## Episodic task [[episodic-task]]\\n\\nIn this case,\\xa0we have a starting point and an ending point\\xa0**(a terminal state).\\xa0This creates an episode**: a list of States, Actions, Rewards, and new States.\\n\\nFor instance, think about Super Mario Bros: an episode begin at the launch of a new Mario Level and ends\\xa0**when youâ€™re killed or you reached the end of the level.**'),\n",
              " Document(id='7b20be6f-f256-4a7b-96e4-f2aa110bd943', metadata={'source': 'optimum', 'start_index': 4706}, page_content='- [GPT2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\\n- [GPT-j](https://huggingface.co/EleutherAI/gpt-j-6B)\\n- [GPT-neo](https://github.com/EleutherAI/gpt-neo)\\n- [GPT-neo-x](https://arxiv.org/abs/2204.06745)'),\n",
              " Document(id='a9258160-b445-4437-9242-8cc290e43858', metadata={'source': 'blog', 'start_index': 12019}, page_content='* *Multiple-Choice*: the model has to compare multiple answer token segments to each other which are usually separated by a significant length\\n* *Summarization*: the model has to learn the relationship between a long sequence of context tokens and a shorter sequence of summary tokens, whereas the relevant relationships between context and summary can most likely not be captured by local self-attention\\n* etc...'),\n",
              " Document(id='4cf62093-8b4f-431c-8372-ae124c72ccd1', metadata={'source': 'blog', 'start_index': 1982}, page_content='[*Pegasus*](https://arxiv.org/abs/1912.08777) on multiple\\n*sequence-to-sequence* tasks at a fraction of the training cost.'),\n",
              " Document(id='8a9eac3f-fa0d-4804-87ae-2c64fdc878c8', metadata={'source': 'blog', 'start_index': 23353}, page_content='You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\\n```')]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectordb.similarity_search(\n",
        "            \"What is a Multi-Task Learning model?\",\n",
        "            k=10,\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "l2YME_53NwbM"
      },
      "outputs": [],
      "source": [
        "from smolagents import Tool\n",
        "from langchain_core.vectorstores import VectorStore\n",
        "\n",
        "\n",
        "class RetrieverTool(Tool):\n",
        "    name = \"retriever\"\n",
        "    description = \"Using semantic similarity, retrieves some documents from the knowledge base that have the closest embeddings to the input query.\"\n",
        "    inputs = {\n",
        "        \"query\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n",
        "        }\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def __init__(self, vectordb: VectorStore, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vectordb = vectordb\n",
        "\n",
        "    def forward(self, query: str) -> str:\n",
        "        assert isinstance(query, str), \"Your search query must be a string\"\n",
        "\n",
        "        docs = self.vectordb.similarity_search(\n",
        "            query,\n",
        "            k=7,\n",
        "        )\n",
        "\n",
        "        return \"\\nRetrieved documents:\\n\" + \"\".join(\n",
        "            [\n",
        "                f\"===== Document {str(i)} =====\\n\" + doc.page_content\n",
        "                for i, doc in enumerate(docs)\n",
        "            ]\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8i9u1VJNwbN"
      },
      "source": [
        "Now itâ€™s straightforward to create an agent that leverages this tool!\n",
        "\n",
        "The agent will need these arguments upon initialization:\n",
        "- *`tools`*: a list of tools that the agent will be able to call.\n",
        "- *`model`*: the LLM that powers the agent.\n",
        "\n",
        "Our `model` must be a callable that takes as input a list of [messages](https://huggingface.co/docs/transformers/main/chat_templating) and returns text. It also needs to accept a `stop_sequences` argument that indicates when to stop its generation. For convenience, we directly use the `HfApiModel` class provided in the package to get a LLM engine that calls our [Inference API](https://huggingface.co/docs/api-inference/en/index).\n",
        "\n",
        "And we use [meta-llama/Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct), served for free on Hugging Face's Inference API!\n",
        "\n",
        "_Note:_ The Inference API hosts models based on various criteria, and deployed models may be updated or replaced without prior notice. Learn more about it [here](https://huggingface.co/docs/api-inference/supported-models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "DO8s8pfeNwbN"
      },
      "outputs": [],
      "source": [
        "from smolagents import HfApiModel, ToolCallingAgent\n",
        "\n",
        "# model = HfApiModel(\"meta-llama/Llama-3.1-70B-Instruct\")\n",
        "model = HfApiModel(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
        "\n",
        "retriever_tool = RetrieverTool(vectordb)\n",
        "agent = ToolCallingAgent(\n",
        "    tools=[retriever_tool], model=model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwHcIYgBNwbN"
      },
      "source": [
        "Since we initialized the agent as a `ReactJsonAgent`, it has been automatically given a default system prompt that tells the LLM engine to process step-by-step and generate tool calls as JSON blobs (you could replace this prompt template with your own as needed).\n",
        "\n",
        "Then when its `.run()` method is launched, the agent takes care of calling the LLM engine, parsing the tool call JSON blobs and executing these tool calls, all in a loop that ends only when the final answer is provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LOlTr31qNwbO",
        "outputId": "d9fa377a-1a72-4138-9df0-cee53b2edabc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">What is a Multi-Task Learning model?</span>                                                                            <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â•°â”€ HfApiModel - mistralai/Mixtral-8x7B-Instruct-v0.1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[38;2;212;183;2mâ•­â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â•®\u001b[0m\n",
              "\u001b[38;2;212;183;2mâ”‚\u001b[0m                                                                                                                 \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
              "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1mWhat is a Multi-Task Learning model?\u001b[0m                                                                            \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
              "\u001b[38;2;212;183;2mâ”‚\u001b[0m                                                                                                                 \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
              "\u001b[38;2;212;183;2mâ•°â”€\u001b[0m\u001b[38;2;212;183;2m HfApiModel - mistralai/Mixtral-8x7B-Instruct-v0.1 \u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â•¯\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[38;2;212;183;2mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚ Calling tool: 'retriever' with arguments: {'query': 'Definition of Multi-Task Learning model'}                  â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚ Calling tool: 'retriever' with arguments: {'query': 'Definition of Multi-Task Learning model'}                  â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
              "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
              "--task TASK           The task to export the model for. If not specified, the task will be auto-inferred based on \n",
              "the model. Available tasks depend on the model, but are among: |<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fill-mask'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'text-generation'</span>, \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'text2text-generation'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'text-classification'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token-classification'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'multiple-choice'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'object-detection'</span>, \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'question-answering'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'image-classification'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'image-segmentation'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'masked-im'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'semantic-segmentation'</span>, \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'automatic-speech-recognition'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'audio-classification'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'audio-frame-classification'</span>, \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'automatic-speech-recognition'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'audio-xvector'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'image-to-text'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'stable-diffusion'</span>, \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'zero-shot-object-detection'</span><span style=\"font-weight: bold\">]</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
              "|Multitask prompt tuning <span style=\"font-weight: bold\">(</span>MPT<span style=\"font-weight: bold\">)](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://hf.co/papers/2103.10385)</span> learns a single prompt from data for multiple task\n",
              "types that can be shared for different target tasks. Other existing approaches learn a separate soft prompt for \n",
              "each task that need to be retrieved or aggregated for adaptation to target tasks. MPT consists of two stages:===== \n",
              "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. source training - for each task, its soft prompt is decomposed into task-specific vectors. The task-specific \n",
              "vectors are multiplied together to form another matrix W, and the Hadamard product is used between W and a shared \n",
              "prompt matrix P to generate a task-specific prompt matrix. The task-specific prompts are distilled into a single \n",
              "prompt matrix that is shared across all tasks. This prompt is trained with multitask training.\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. target adaptation - to adapt the single prompt for a target task, a target prompt is initialized and expressed \n",
              "as the Hadamard product of the shared prompt matrix and the task-specific low-rank prompt matrix.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> \n",
              "=====\n",
              "### Q4: A task is an instance of a Reinforcement Learning problem. What are the two types of tasks?===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>\n",
              "=====\n",
              "Optional arguments:\n",
              "  --task TASK           The task to export the model for. If not specified, the task will be auto-inferred based on\n",
              "the model. Available tasks depend on\n",
              "                        the model, but are among: |<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fill-mask'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'text-generation'</span>, \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'text2text-generation'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'text-classification'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token-classification'</span>,\n",
              "                        <span style=\"color: #008000; text-decoration-color: #008000\">'multiple-choice'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'object-detection'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'question-answering'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'image-classification'</span>, \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'image-segmentation'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'masked-im'</span>, 'semantic-\n",
              "                        segmentation', <span style=\"color: #008000; text-decoration-color: #008000\">'automatic-speech-recognition'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'audio-classification'</span>, \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'audio-frame-classification'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'automatic-speech-recognition'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'audio-xvector'</span>, 'vision2seq-===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
              "--&gt;\n",
              "\n",
              "# MVP\n",
              "\n",
              "## Overview\n",
              "\n",
              "The MVP model was proposed in |MVP: Multi-task Supervised Pre-training for Natural Language \n",
              "Generation<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2206.12131)</span> by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.\n",
              "\n",
              "\n",
              "According to the abstract,===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
              "- \n",
              "|GPT2<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learne</span>\n",
              "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">rs.pdf)</span>\n",
              "- |GPT-j<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/EleutherAI/gpt-j-6B)</span>\n",
              "- |GPT-neo<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/EleutherAI/gpt-neo)</span>\n",
              "- |GPT-neo-x<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2204.06745)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Observations: Retrieved documents:\n",
              "===== Document \u001b[1;36m0\u001b[0m =====\n",
              "--task TASK           The task to export the model for. If not specified, the task will be auto-inferred based on \n",
              "the model. Available tasks depend on the model, but are among: |\u001b[32m'default'\u001b[0m, \u001b[32m'fill-mask'\u001b[0m, \u001b[32m'text-generation'\u001b[0m, \n",
              "\u001b[32m'text2text-generation'\u001b[0m, \u001b[32m'text-classification'\u001b[0m, \u001b[32m'token-classification'\u001b[0m, \u001b[32m'multiple-choice'\u001b[0m, \u001b[32m'object-detection'\u001b[0m, \n",
              "\u001b[32m'question-answering'\u001b[0m, \u001b[32m'image-classification'\u001b[0m, \u001b[32m'image-segmentation'\u001b[0m, \u001b[32m'masked-im'\u001b[0m, \u001b[32m'semantic-segmentation'\u001b[0m, \n",
              "\u001b[32m'automatic-speech-recognition'\u001b[0m, \u001b[32m'audio-classification'\u001b[0m, \u001b[32m'audio-frame-classification'\u001b[0m, \n",
              "\u001b[32m'automatic-speech-recognition'\u001b[0m, \u001b[32m'audio-xvector'\u001b[0m, \u001b[32m'image-to-text'\u001b[0m, \u001b[32m'stable-diffusion'\u001b[0m, \n",
              "\u001b[32m'zero-shot-object-detection'\u001b[0m\u001b[1m]\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
              "|Multitask prompt tuning \u001b[1m(\u001b[0mMPT\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://hf.co/papers/2103.10385\u001b[0m\u001b[4;94m)\u001b[0m learns a single prompt from data for multiple task\n",
              "types that can be shared for different target tasks. Other existing approaches learn a separate soft prompt for \n",
              "each task that need to be retrieved or aggregated for adaptation to target tasks. MPT consists of two stages:===== \n",
              "Document \u001b[1;36m2\u001b[0m =====\n",
              "\u001b[1;36m1\u001b[0m. source training - for each task, its soft prompt is decomposed into task-specific vectors. The task-specific \n",
              "vectors are multiplied together to form another matrix W, and the Hadamard product is used between W and a shared \n",
              "prompt matrix P to generate a task-specific prompt matrix. The task-specific prompts are distilled into a single \n",
              "prompt matrix that is shared across all tasks. This prompt is trained with multitask training.\n",
              "\u001b[1;36m2\u001b[0m. target adaptation - to adapt the single prompt for a target task, a target prompt is initialized and expressed \n",
              "as the Hadamard product of the shared prompt matrix and the task-specific low-rank prompt matrix.===== Document \u001b[1;36m3\u001b[0m \n",
              "=====\n",
              "### Q4: A task is an instance of a Reinforcement Learning problem. What are the two types of tasks?===== Document \u001b[1;36m4\u001b[0m\n",
              "=====\n",
              "Optional arguments:\n",
              "  --task TASK           The task to export the model for. If not specified, the task will be auto-inferred based on\n",
              "the model. Available tasks depend on\n",
              "                        the model, but are among: |\u001b[32m'default'\u001b[0m, \u001b[32m'fill-mask'\u001b[0m, \u001b[32m'text-generation'\u001b[0m, \n",
              "\u001b[32m'text2text-generation'\u001b[0m, \u001b[32m'text-classification'\u001b[0m, \u001b[32m'token-classification'\u001b[0m,\n",
              "                        \u001b[32m'multiple-choice'\u001b[0m, \u001b[32m'object-detection'\u001b[0m, \u001b[32m'question-answering'\u001b[0m, \u001b[32m'image-classification'\u001b[0m, \n",
              "\u001b[32m'image-segmentation'\u001b[0m, \u001b[32m'masked-im'\u001b[0m, 'semantic-\n",
              "                        segmentation', \u001b[32m'automatic-speech-recognition'\u001b[0m, \u001b[32m'audio-classification'\u001b[0m, \n",
              "\u001b[32m'audio-frame-classification'\u001b[0m, \u001b[32m'automatic-speech-recognition'\u001b[0m, \u001b[32m'audio-xvector'\u001b[0m, 'vision2seq-===== Document \u001b[1;36m5\u001b[0m =====\n",
              "-->\n",
              "\n",
              "# MVP\n",
              "\n",
              "## Overview\n",
              "\n",
              "The MVP model was proposed in |MVP: Multi-task Supervised Pre-training for Natural Language \n",
              "Generation\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2206.12131\u001b[0m\u001b[4;94m)\u001b[0m by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.\n",
              "\n",
              "\n",
              "According to the abstract,===== Document \u001b[1;36m6\u001b[0m =====\n",
              "- \n",
              "|GPT2\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learne\u001b[0m\n",
              "\u001b[4;94mrs.pdf\u001b[0m\u001b[4;94m)\u001b[0m\n",
              "- |GPT-j\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/EleutherAI/gpt-j-6B\u001b[0m\u001b[4;94m)\u001b[0m\n",
              "- |GPT-neo\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/EleutherAI/gpt-neo\u001b[0m\u001b[4;94m)\u001b[0m\n",
              "- |GPT-neo-x\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2204.06745\u001b[0m\u001b[4;94m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 5.93 seconds| Input tokens: 1,325 | Output tokens: 27]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2m[Step 1: Duration 5.93 seconds| Input tokens: 1,325 | Output tokens: 27]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[38;2;212;183;2mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚ Calling tool: 'retriever' with arguments: {'query': 'Definition of Multi-Task Learning model in NLP'}           â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚ Calling tool: 'retriever' with arguments: {'query': 'Definition of Multi-Task Learning model in NLP'}           â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
              "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
              "## M\n",
              "\n",
              "### masked language modeling <span style=\"font-weight: bold\">(</span>MLM<span style=\"font-weight: bold\">)</span>\n",
              "\n",
              "A pretraining task where the model sees a corrupted version of the texts, usually done by\n",
              "masking some tokens randomly, and has to predict the original text.\n",
              "\n",
              "### multimodal\n",
              "\n",
              "A task that combines texts with another kind of inputs <span style=\"font-weight: bold\">(</span>for instance images<span style=\"font-weight: bold\">)</span>.\n",
              "\n",
              "## N\n",
              "\n",
              "### Natural language generation <span style=\"font-weight: bold\">(</span>NLG<span style=\"font-weight: bold\">)</span>\n",
              "\n",
              "All tasks related to generating text <span style=\"font-weight: bold\">(</span>for instance, |Write With Transformers<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://transformer.huggingface.co/),</span>\n",
              "translation<span style=\"font-weight: bold\">)</span>.\n",
              "\n",
              "### Natural language processing <span style=\"font-weight: bold\">(</span>NLP<span style=\"font-weight: bold\">)</span>\n",
              "\n",
              "A generic way to say <span style=\"color: #008000; text-decoration-color: #008000\">\"deal with texts\"</span>.\n",
              "\n",
              "### Natural language understanding <span style=\"font-weight: bold\">(</span>NLU<span style=\"font-weight: bold\">)</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
              "--task TASK           The task to export the model for. If not specified, the task will be auto-inferred based on \n",
              "the model. Available tasks depend on the model, but are among: |<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fill-mask'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'text-generation'</span>, \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'text2text-generation'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'text-classification'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token-classification'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'multiple-choice'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'object-detection'</span>, \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'question-answering'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'image-classification'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'image-segmentation'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'masked-im'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'semantic-segmentation'</span>, \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'automatic-speech-recognition'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'audio-classification'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'audio-frame-classification'</span>, \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'automatic-speech-recognition'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'audio-xvector'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'image-to-text'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'stable-diffusion'</span>, \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'zero-shot-object-detection'</span><span style=\"font-weight: bold\">]</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
              "--&gt;\n",
              "\n",
              "# MVP\n",
              "\n",
              "## Overview\n",
              "\n",
              "The MVP model was proposed in |MVP: Multi-task Supervised Pre-training for Natural Language \n",
              "Generation<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2206.12131)</span> by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.\n",
              "\n",
              "\n",
              "According to the abstract,===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
              "The following is a list of common NLP tasks, with some examples of each:\n",
              "\n",
              "- **Classifying whole sentences**: Getting the sentiment of a review, detecting if an email is spam, determining if\n",
              "a sentence is grammatically correct or whether two sentences are logically related or not\n",
              "- **Classifying each word in a sentence**: Identifying the grammatical components of a sentence <span style=\"font-weight: bold\">(</span>noun, verb, \n",
              "adjective<span style=\"font-weight: bold\">)</span>, or the named entities <span style=\"font-weight: bold\">(</span>person, location, organization<span style=\"font-weight: bold\">)</span>\n",
              "- **Generating text content**: Completing a prompt with auto-generated text, filling in the blanks in a text with \n",
              "masked words\n",
              "- **Extracting an answer from a text**: Given a question and a context, extracting the answer to the question based\n",
              "on the information provided in the context\n",
              "- **Generating a new sentence from an input text**: Translating a text into another language, summarizing a \n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">text</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
              "|Radford et al.\n",
              "<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2019</span><span style=\"font-weight: bold\">)](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf),</span>\n",
              "the authors show that a pre-trained GPT2 model can achieve SOTA or close\n",
              "to SOTA results on a variety of NLG tasks without much fine-tuning. All\n",
              "*auto-regressive* models of ğŸ¤—Transformers can be found\n",
              "|here<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/transformers/model_summary.html#autoregressive-models).=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
              "--&gt;\n",
              "\n",
              "# Image tasks with IDEFICS\n",
              "\n",
              "||open-in-colab<span style=\"font-weight: bold\">]]</span>\n",
              "\n",
              "While individual tasks can be tackled by fine-tuning specialized models, an alternative approach \n",
              "that has recently emerged and gained popularity is to use large models for a diverse set of tasks without \n",
              "fine-tuning. \n",
              "For instance, large language models can handle such NLP tasks as summarization, translation, classification, and \n",
              "more. \n",
              "This approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how \n",
              "you can \n",
              "solve image-text tasks with a large multimodal model called IDEFICS.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
              "- \n",
              "|GPT2<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learne</span>\n",
              "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">rs.pdf)</span>\n",
              "- |GPT-j<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/EleutherAI/gpt-j-6B)</span>\n",
              "- |GPT-neo<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/EleutherAI/gpt-neo)</span>\n",
              "- |GPT-neo-x<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2204.06745)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Observations: Retrieved documents:\n",
              "===== Document \u001b[1;36m0\u001b[0m =====\n",
              "## M\n",
              "\n",
              "### masked language modeling \u001b[1m(\u001b[0mMLM\u001b[1m)\u001b[0m\n",
              "\n",
              "A pretraining task where the model sees a corrupted version of the texts, usually done by\n",
              "masking some tokens randomly, and has to predict the original text.\n",
              "\n",
              "### multimodal\n",
              "\n",
              "A task that combines texts with another kind of inputs \u001b[1m(\u001b[0mfor instance images\u001b[1m)\u001b[0m.\n",
              "\n",
              "## N\n",
              "\n",
              "### Natural language generation \u001b[1m(\u001b[0mNLG\u001b[1m)\u001b[0m\n",
              "\n",
              "All tasks related to generating text \u001b[1m(\u001b[0mfor instance, |Write With Transformers\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://transformer.huggingface.co/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m\n",
              "translation\u001b[1m)\u001b[0m.\n",
              "\n",
              "### Natural language processing \u001b[1m(\u001b[0mNLP\u001b[1m)\u001b[0m\n",
              "\n",
              "A generic way to say \u001b[32m\"deal with texts\"\u001b[0m.\n",
              "\n",
              "### Natural language understanding \u001b[1m(\u001b[0mNLU\u001b[1m)\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
              "--task TASK           The task to export the model for. If not specified, the task will be auto-inferred based on \n",
              "the model. Available tasks depend on the model, but are among: |\u001b[32m'default'\u001b[0m, \u001b[32m'fill-mask'\u001b[0m, \u001b[32m'text-generation'\u001b[0m, \n",
              "\u001b[32m'text2text-generation'\u001b[0m, \u001b[32m'text-classification'\u001b[0m, \u001b[32m'token-classification'\u001b[0m, \u001b[32m'multiple-choice'\u001b[0m, \u001b[32m'object-detection'\u001b[0m, \n",
              "\u001b[32m'question-answering'\u001b[0m, \u001b[32m'image-classification'\u001b[0m, \u001b[32m'image-segmentation'\u001b[0m, \u001b[32m'masked-im'\u001b[0m, \u001b[32m'semantic-segmentation'\u001b[0m, \n",
              "\u001b[32m'automatic-speech-recognition'\u001b[0m, \u001b[32m'audio-classification'\u001b[0m, \u001b[32m'audio-frame-classification'\u001b[0m, \n",
              "\u001b[32m'automatic-speech-recognition'\u001b[0m, \u001b[32m'audio-xvector'\u001b[0m, \u001b[32m'image-to-text'\u001b[0m, \u001b[32m'stable-diffusion'\u001b[0m, \n",
              "\u001b[32m'zero-shot-object-detection'\u001b[0m\u001b[1m]\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
              "-->\n",
              "\n",
              "# MVP\n",
              "\n",
              "## Overview\n",
              "\n",
              "The MVP model was proposed in |MVP: Multi-task Supervised Pre-training for Natural Language \n",
              "Generation\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2206.12131\u001b[0m\u001b[4;94m)\u001b[0m by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.\n",
              "\n",
              "\n",
              "According to the abstract,===== Document \u001b[1;36m3\u001b[0m =====\n",
              "The following is a list of common NLP tasks, with some examples of each:\n",
              "\n",
              "- **Classifying whole sentences**: Getting the sentiment of a review, detecting if an email is spam, determining if\n",
              "a sentence is grammatically correct or whether two sentences are logically related or not\n",
              "- **Classifying each word in a sentence**: Identifying the grammatical components of a sentence \u001b[1m(\u001b[0mnoun, verb, \n",
              "adjective\u001b[1m)\u001b[0m, or the named entities \u001b[1m(\u001b[0mperson, location, organization\u001b[1m)\u001b[0m\n",
              "- **Generating text content**: Completing a prompt with auto-generated text, filling in the blanks in a text with \n",
              "masked words\n",
              "- **Extracting an answer from a text**: Given a question and a context, extracting the answer to the question based\n",
              "on the information provided in the context\n",
              "- **Generating a new sentence from an input text**: Translating a text into another language, summarizing a \n",
              "\u001b[33mtext\u001b[0m===== Document \u001b[1;36m4\u001b[0m =====\n",
              "|Radford et al.\n",
              "\u001b[1m(\u001b[0m\u001b[1;36m2019\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m\n",
              "the authors show that a pre-trained GPT2 model can achieve SOTA or close\n",
              "to SOTA results on a variety of NLG tasks without much fine-tuning. All\n",
              "*auto-regressive* models of ğŸ¤—Transformers can be found\n",
              "|here\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/transformers/model_summary.html#autoregressive-models\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m Document \u001b[1;36m5\u001b[0m =====\n",
              "-->\n",
              "\n",
              "# Image tasks with IDEFICS\n",
              "\n",
              "||open-in-colab\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n",
              "\n",
              "While individual tasks can be tackled by fine-tuning specialized models, an alternative approach \n",
              "that has recently emerged and gained popularity is to use large models for a diverse set of tasks without \n",
              "fine-tuning. \n",
              "For instance, large language models can handle such NLP tasks as summarization, translation, classification, and \n",
              "more. \n",
              "This approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how \n",
              "you can \n",
              "solve image-text tasks with a large multimodal model called IDEFICS.===== Document \u001b[1;36m6\u001b[0m =====\n",
              "- \n",
              "|GPT2\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learne\u001b[0m\n",
              "\u001b[4;94mrs.pdf\u001b[0m\u001b[4;94m)\u001b[0m\n",
              "- |GPT-j\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/EleutherAI/gpt-j-6B\u001b[0m\u001b[4;94m)\u001b[0m\n",
              "- |GPT-neo\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/EleutherAI/gpt-neo\u001b[0m\u001b[4;94m)\u001b[0m\n",
              "- |GPT-neo-x\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2204.06745\u001b[0m\u001b[4;94m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 0.82 seconds| Input tokens: 3,609 | Output tokens: 58]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2m[Step 2: Duration 0.82 seconds| Input tokens: 3,609 | Output tokens: 58]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[38;2;212;183;2mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚ Calling tool: 'retriever' with arguments: {'query': 'Definition of Multi-Task Learning model in NLP and its     â”‚\n",
              "â”‚ relation to Natural Language Generation tasks'}                                                                 â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚ Calling tool: 'retriever' with arguments: {'query': 'Definition of Multi-Task Learning model in NLP and its     â”‚\n",
              "â”‚ relation to Natural Language Generation tasks'}                                                                 â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
              "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
              "The following is a list of common NLP tasks, with some examples of each:\n",
              "\n",
              "- **Classifying whole sentences**: Getting the sentiment of a review, detecting if an email is spam, determining if\n",
              "a sentence is grammatically correct or whether two sentences are logically related or not\n",
              "- **Classifying each word in a sentence**: Identifying the grammatical components of a sentence <span style=\"font-weight: bold\">(</span>noun, verb, \n",
              "adjective<span style=\"font-weight: bold\">)</span>, or the named entities <span style=\"font-weight: bold\">(</span>person, location, organization<span style=\"font-weight: bold\">)</span>\n",
              "- **Generating text content**: Completing a prompt with auto-generated text, filling in the blanks in a text with \n",
              "masked words\n",
              "- **Extracting an answer from a text**: Given a question and a context, extracting the answer to the question based\n",
              "on the information provided in the context\n",
              "- **Generating a new sentence from an input text**: Translating a text into another language, summarizing a \n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">text</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
              "## M\n",
              "\n",
              "### masked language modeling <span style=\"font-weight: bold\">(</span>MLM<span style=\"font-weight: bold\">)</span>\n",
              "\n",
              "A pretraining task where the model sees a corrupted version of the texts, usually done by\n",
              "masking some tokens randomly, and has to predict the original text.\n",
              "\n",
              "### multimodal\n",
              "\n",
              "A task that combines texts with another kind of inputs <span style=\"font-weight: bold\">(</span>for instance images<span style=\"font-weight: bold\">)</span>.\n",
              "\n",
              "## N\n",
              "\n",
              "### Natural language generation <span style=\"font-weight: bold\">(</span>NLG<span style=\"font-weight: bold\">)</span>\n",
              "\n",
              "All tasks related to generating text <span style=\"font-weight: bold\">(</span>for instance, |Write With Transformers<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://transformer.huggingface.co/),</span>\n",
              "translation<span style=\"font-weight: bold\">)</span>.\n",
              "\n",
              "### Natural language processing <span style=\"font-weight: bold\">(</span>NLP<span style=\"font-weight: bold\">)</span>\n",
              "\n",
              "A generic way to say <span style=\"color: #008000; text-decoration-color: #008000\">\"deal with texts\"</span>.\n",
              "\n",
              "### Natural language understanding <span style=\"font-weight: bold\">(</span>NLU<span style=\"font-weight: bold\">)</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
              "Natural Language Processing||natural-language-processing<span style=\"font-weight: bold\">]]</span>\n",
              "\n",
              "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">CourseFloatingBanner</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">chapter</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">classNames</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"absolute z-10 right-0 top-0\"</span>\n",
              "<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
              "\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">Before jumping into Transformer models, let's do a quick overview of what natural language processing is and why we</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">care about it.</span>\n",
              "\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">## What is NLP?||what-is-nlp</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]]</span>\n",
              "\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">NLP is a field of linguistics and machine learning focused on understanding everything related to human language. </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">The aim of NLP tasks is not only to understand single words individually, but to be able to understand the context </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">of those words.</span>\n",
              "\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">The following is a list of common NLP tasks, with some examples of each:===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">--</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "\n",
              "# MVP\n",
              "\n",
              "## Overview\n",
              "\n",
              "The MVP model was proposed in |MVP: Multi-task Supervised Pre-training for Natural Language \n",
              "Generation<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2206.12131)</span> by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.\n",
              "\n",
              "\n",
              "According to the abstract,===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
              ".microsoft.com/en-us/research/publication/scalable-and-efficient-moe-training-for-multitask-multilingual-models/<span style=\"font-weight: bold\">)</span> \n",
              "and specific deployment with large transformer-based natural language generation models: |blog \n",
              "post<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.deepspeed.ai/2021/12/09/deepspeed-moe-nlg.html),</span> |Megatron-Deepspeed \n",
              "branch<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/microsoft/Megatron-DeepSpeed/tree/moe-training).=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
              "## Natural language processing\n",
              "\n",
              "NLP tasks are among the most common types of tasks because text is such a natural way for us to communicate. To get\n",
              "text into a format recognized by a model, it needs to be tokenized. This means dividing a sequence of text into \n",
              "separate words or subwords <span style=\"font-weight: bold\">(</span>tokens<span style=\"font-weight: bold\">)</span> and then converting these tokens into numbers. As a result, you can represent a\n",
              "sequence of text as a sequence of numbers, and once you have a sequence of numbers, it can be input into a model to\n",
              "solve all sorts of NLP tasks!\n",
              "\n",
              "### Text classification\n",
              "\n",
              "Like classification tasks in any modality, text classification labels a sequence of text <span style=\"font-weight: bold\">(</span>it can be sentence-level,\n",
              "a paragraph, or a document<span style=\"font-weight: bold\">)</span> from a predefined set of classes. There are many practical applications for text \n",
              "classification, some of which include:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
              "--task TASK           The task to export the model for. If not specified, the task will be auto-inferred based on \n",
              "the model. Available tasks depend on the model, but are among: |<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fill-mask'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'text-generation'</span>, \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'text2text-generation'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'text-classification'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token-classification'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'multiple-choice'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'object-detection'</span>, \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'question-answering'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'image-classification'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'image-segmentation'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'masked-im'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'semantic-segmentation'</span>, \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'automatic-speech-recognition'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'audio-classification'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'audio-frame-classification'</span>, \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'automatic-speech-recognition'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'audio-xvector'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'image-to-text'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'stable-diffusion'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'zero-shot-object-detection'</span><span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Observations: Retrieved documents:\n",
              "===== Document \u001b[1;36m0\u001b[0m =====\n",
              "The following is a list of common NLP tasks, with some examples of each:\n",
              "\n",
              "- **Classifying whole sentences**: Getting the sentiment of a review, detecting if an email is spam, determining if\n",
              "a sentence is grammatically correct or whether two sentences are logically related or not\n",
              "- **Classifying each word in a sentence**: Identifying the grammatical components of a sentence \u001b[1m(\u001b[0mnoun, verb, \n",
              "adjective\u001b[1m)\u001b[0m, or the named entities \u001b[1m(\u001b[0mperson, location, organization\u001b[1m)\u001b[0m\n",
              "- **Generating text content**: Completing a prompt with auto-generated text, filling in the blanks in a text with \n",
              "masked words\n",
              "- **Extracting an answer from a text**: Given a question and a context, extracting the answer to the question based\n",
              "on the information provided in the context\n",
              "- **Generating a new sentence from an input text**: Translating a text into another language, summarizing a \n",
              "\u001b[33mtext\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
              "## M\n",
              "\n",
              "### masked language modeling \u001b[1m(\u001b[0mMLM\u001b[1m)\u001b[0m\n",
              "\n",
              "A pretraining task where the model sees a corrupted version of the texts, usually done by\n",
              "masking some tokens randomly, and has to predict the original text.\n",
              "\n",
              "### multimodal\n",
              "\n",
              "A task that combines texts with another kind of inputs \u001b[1m(\u001b[0mfor instance images\u001b[1m)\u001b[0m.\n",
              "\n",
              "## N\n",
              "\n",
              "### Natural language generation \u001b[1m(\u001b[0mNLG\u001b[1m)\u001b[0m\n",
              "\n",
              "All tasks related to generating text \u001b[1m(\u001b[0mfor instance, |Write With Transformers\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://transformer.huggingface.co/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m\n",
              "translation\u001b[1m)\u001b[0m.\n",
              "\n",
              "### Natural language processing \u001b[1m(\u001b[0mNLP\u001b[1m)\u001b[0m\n",
              "\n",
              "A generic way to say \u001b[32m\"deal with texts\"\u001b[0m.\n",
              "\n",
              "### Natural language understanding \u001b[1m(\u001b[0mNLU\u001b[1m)\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
              "Natural Language Processing||natural-language-processing\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n",
              "\n",
              "\u001b[1m<\u001b[0m\u001b[1;95mCourseFloatingBanner\u001b[0m\n",
              "\u001b[39m    \u001b[0m\u001b[33mchapter\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;39m}\u001b[0m\n",
              "\u001b[39m    \u001b[0m\u001b[33mclassNames\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"absolute\u001b[0m\u001b[32m z-10 right-0 top-0\"\u001b[0m\n",
              "\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
              "\n",
              "\u001b[39mBefore jumping into Transformer models, let's do a quick overview of what natural language processing is and why we\u001b[0m\n",
              "\u001b[39mcare about it.\u001b[0m\n",
              "\n",
              "\u001b[39m## What is NLP?||what-is-nlp\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m]\u001b[0m\n",
              "\n",
              "\u001b[39mNLP is a field of linguistics and machine learning focused on understanding everything related to human language. \u001b[0m\n",
              "\u001b[39mThe aim of NLP tasks is not only to understand single words individually, but to be able to understand the context \u001b[0m\n",
              "\u001b[39mof those words.\u001b[0m\n",
              "\n",
              "\u001b[39mThe following is a list of common NLP tasks, with some examples of each:===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
              "\u001b[39m--\u001b[0m\u001b[1m>\u001b[0m\n",
              "\n",
              "# MVP\n",
              "\n",
              "## Overview\n",
              "\n",
              "The MVP model was proposed in |MVP: Multi-task Supervised Pre-training for Natural Language \n",
              "Generation\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2206.12131\u001b[0m\u001b[4;94m)\u001b[0m by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.\n",
              "\n",
              "\n",
              "According to the abstract,===== Document \u001b[1;36m4\u001b[0m =====\n",
              ".microsoft.com/en-us/research/publication/scalable-and-efficient-moe-training-for-multitask-multilingual-models/\u001b[1m)\u001b[0m \n",
              "and specific deployment with large transformer-based natural language generation models: |blog \n",
              "post\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://www.deepspeed.ai/2021/12/09/deepspeed-moe-nlg.html\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m |Megatron-Deepspeed \n",
              "branch\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/microsoft/Megatron-DeepSpeed/tree/moe-training\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m Document \u001b[1;36m5\u001b[0m =====\n",
              "## Natural language processing\n",
              "\n",
              "NLP tasks are among the most common types of tasks because text is such a natural way for us to communicate. To get\n",
              "text into a format recognized by a model, it needs to be tokenized. This means dividing a sequence of text into \n",
              "separate words or subwords \u001b[1m(\u001b[0mtokens\u001b[1m)\u001b[0m and then converting these tokens into numbers. As a result, you can represent a\n",
              "sequence of text as a sequence of numbers, and once you have a sequence of numbers, it can be input into a model to\n",
              "solve all sorts of NLP tasks!\n",
              "\n",
              "### Text classification\n",
              "\n",
              "Like classification tasks in any modality, text classification labels a sequence of text \u001b[1m(\u001b[0mit can be sentence-level,\n",
              "a paragraph, or a document\u001b[1m)\u001b[0m from a predefined set of classes. There are many practical applications for text \n",
              "classification, some of which include:===== Document \u001b[1;36m6\u001b[0m =====\n",
              "--task TASK           The task to export the model for. If not specified, the task will be auto-inferred based on \n",
              "the model. Available tasks depend on the model, but are among: |\u001b[32m'default'\u001b[0m, \u001b[32m'fill-mask'\u001b[0m, \u001b[32m'text-generation'\u001b[0m, \n",
              "\u001b[32m'text2text-generation'\u001b[0m, \u001b[32m'text-classification'\u001b[0m, \u001b[32m'token-classification'\u001b[0m, \u001b[32m'multiple-choice'\u001b[0m, \u001b[32m'object-detection'\u001b[0m, \n",
              "\u001b[32m'question-answering'\u001b[0m, \u001b[32m'image-classification'\u001b[0m, \u001b[32m'image-segmentation'\u001b[0m, \u001b[32m'masked-im'\u001b[0m, \u001b[32m'semantic-segmentation'\u001b[0m, \n",
              "\u001b[32m'automatic-speech-recognition'\u001b[0m, \u001b[32m'audio-classification'\u001b[0m, \u001b[32m'audio-frame-classification'\u001b[0m, \n",
              "\u001b[32m'automatic-speech-recognition'\u001b[0m, \u001b[32m'audio-xvector'\u001b[0m, \u001b[32m'image-to-text'\u001b[0m, \u001b[32m'stable-diffusion'\u001b[0m, \u001b[32m'zero-shot-object-detection'\u001b[0m\u001b[1m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 1.08 seconds| Input tokens: 7,061 | Output tokens: 97]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2m[Step 3: Duration 1.08 seconds| Input tokens: 7,061 | Output tokens: 97]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[38;2;212;183;2mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m4\u001b[0m\u001b[38;2;212;183;2m â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚ Calling tool: 'final_answer' with arguments: {'answer': 'A Multi-Task Learning model in Natural Language        â”‚\n",
              "â”‚ Generation (NLG) is a type of model that can handle multiple tasks related to generating text such as           â”‚\n",
              "â”‚ translation, summarization, and filling in blanks, through multi-task supervised pre-training.'}                â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚ Calling tool: 'final_answer' with arguments: {'answer': 'A Multi-Task Learning model in Natural Language        â”‚\n",
              "â”‚ Generation (NLG) is a type of model that can handle multiple tasks related to generating text such as           â”‚\n",
              "â”‚ translation, summarization, and filling in blanks, through multi-task supervised pre-training.'}                â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: A Multi-Task Learning model in Natural Language Generation (NLG) is a type of model that can handle </span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">multiple tasks related to generating text such as translation, summarization, and filling in blanks, through </span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">multi-task supervised pre-training.</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;212;183;2mFinal answer: A Multi-Task Learning model in Natural Language Generation (NLG) is a type of model that can handle \u001b[0m\n",
              "\u001b[1;38;2;212;183;2mmultiple tasks related to generating text such as translation, summarization, and filling in blanks, through \u001b[0m\n",
              "\u001b[1;38;2;212;183;2mmulti-task supervised pre-training.\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 4: Duration 1.42 seconds| Input tokens: 11,734 | Output tokens: 171]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2m[Step 4: Duration 1.42 seconds| Input tokens: 11,734 | Output tokens: 171]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final output:\n",
            "A Multi-Task Learning model in Natural Language Generation (NLG) is a type of model that can handle multiple tasks related to generating text such as translation, summarization, and filling in blanks, through multi-task supervised pre-training.\n"
          ]
        }
      ],
      "source": [
        "agent_output = agent.run(\"What is a Multi-Task Learning model?\")\n",
        "\n",
        "print(\"Final output:\")\n",
        "print(agent_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9ykUH39NwbO"
      },
      "source": [
        "## Agentic RAG vs. standard RAG\n",
        "\n",
        "Does the agent setup make a better RAG system? Well, let's compare it to a standard RAG system using LLM Judge!\n",
        "\n",
        "We will use [meta-llama/Meta-Llama-3-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) for evaluation since it's one of the strongest OS models we tested for LLM judge use cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dDKJR8imNwbO"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff904bc0aa674520948bad92ba2d9e8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/893 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "ReadTimeout",
          "evalue": "(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 8baa6922-a6ce-4330-a752-bc907107c544)')",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m eval_dataset = \u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mm-ric/huggingface_doc_qa_eval\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/datasets/load.py:2061\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2056\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   2057\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   2058\u001b[39m )\n\u001b[32m   2060\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2061\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2062\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2078\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   2079\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/datasets/load.py:1781\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m     download_config = download_config.copy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[32m   1780\u001b[39m     download_config.storage_options.update(storage_options)\n\u001b[32m-> \u001b[39m\u001b[32m1781\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1782\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1787\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1790\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1792\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1793\u001b[39m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[32m   1794\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/datasets/load.py:1663\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[39m\n\u001b[32m   1658\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1659\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1660\u001b[39m                         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1661\u001b[39m                         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1662\u001b[39m                     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1663\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1664\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m trust_remote_code:\n\u001b[32m   1665\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1666\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or any data file in the same directory.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1667\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/datasets/load.py:1628\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[39m\n\u001b[32m   1618\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1619\u001b[39m         use_exported_dataset_infos = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1620\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHubDatasetModuleFactoryWithoutScript\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1627\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_exported_dataset_infos\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_exported_dataset_infos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1628\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1630\u001b[39m     message = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is a gated dataset on the Hub.\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/datasets/load.py:1018\u001b[39m, in \u001b[36mHubDatasetModuleFactoryWithoutScript.get_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1017\u001b[39m     patterns = get_data_patterns(base_path, download_config=\u001b[38;5;28mself\u001b[39m.download_config)\n\u001b[32m-> \u001b[39m\u001b[32m1018\u001b[39m data_files = \u001b[43mDataFilesDict\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mALL_ALLOWED_EXTENSIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1024\u001b[39m module_name, default_builder_kwargs = infer_module_for_data_files(\n\u001b[32m   1025\u001b[39m     data_files=data_files,\n\u001b[32m   1026\u001b[39m     path=\u001b[38;5;28mself\u001b[39m.name,\n\u001b[32m   1027\u001b[39m     download_config=\u001b[38;5;28mself\u001b[39m.download_config,\n\u001b[32m   1028\u001b[39m )\n\u001b[32m   1029\u001b[39m data_files = data_files.filter(\n\u001b[32m   1030\u001b[39m     extensions=_MODULE_TO_EXTENSIONS[module_name], file_names=_MODULE_TO_METADATA_FILE_NAMES[module_name]\n\u001b[32m   1031\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/datasets/data_files.py:690\u001b[39m, in \u001b[36mDataFilesDict.from_patterns\u001b[39m\u001b[34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[39m\n\u001b[32m    685\u001b[39m out = \u001b[38;5;28mcls\u001b[39m()\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, patterns_for_key \u001b[38;5;129;01min\u001b[39;00m patterns.items():\n\u001b[32m    687\u001b[39m     out[key] = (\n\u001b[32m    688\u001b[39m         patterns_for_key\n\u001b[32m    689\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(patterns_for_key, DataFilesList)\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mDataFilesList\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpatterns_for_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    696\u001b[39m     )\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/datasets/data_files.py:593\u001b[39m, in \u001b[36mDataFilesList.from_patterns\u001b[39m\u001b[34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[39m\n\u001b[32m    591\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_magic(pattern):\n\u001b[32m    592\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m origin_metadata = \u001b[43m_get_origin_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(data_files, origin_metadata)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/datasets/data_files.py:507\u001b[39m, in \u001b[36m_get_origin_metadata\u001b[39m\u001b[34m(data_files, download_config, max_workers)\u001b[39m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_origin_metadata\u001b[39m(\n\u001b[32m    502\u001b[39m     data_files: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m    503\u001b[39m     download_config: Optional[DownloadConfig] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    504\u001b[39m     max_workers: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    505\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[SingleOriginMetadata]:\n\u001b[32m    506\u001b[39m     max_workers = max_workers \u001b[38;5;28;01mif\u001b[39;00m max_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m config.HF_DATASETS_MULTITHREADING_MAX_WORKERS\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_get_single_origin_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mResolving data files\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# set `disable=None` rather than `disable=False` by default to disable progress bar when no TTY attached\u001b[39;49;00m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/tqdm/contrib/concurrent.py:69\u001b[39m, in \u001b[36mthread_map\u001b[39m\u001b[34m(fn, *iterables, **tqdm_kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[33;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m \u001b[33;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconcurrent\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfutures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/tqdm/contrib/concurrent.py:51\u001b[39m, in \u001b[36m_executor_map\u001b[39m\u001b[34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name=lock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers=max_workers, initializer=tqdm_class.set_lock,\n\u001b[32m     50\u001b[39m                       initargs=(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/tqdm/notebook.py:250\u001b[39m, in \u001b[36mtqdm_notebook.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    249\u001b[39m     it = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__iter__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/tqdm/std.py:1169\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1166\u001b[39m \u001b[38;5;66;03m# If the bar is disabled, then just walk the iterable\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;66;03m# (note: keep this check outside the loop for performance)\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.disable:\n\u001b[32m-> \u001b[39m\u001b[32m1169\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1171\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/datasets/data_files.py:486\u001b[39m, in \u001b[36m_get_single_origin_metadata\u001b[39m\u001b[34m(data_file, download_config)\u001b[39m\n\u001b[32m    484\u001b[39m fs, *_ = url_to_fs(data_file, **storage_options)\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fs, HfFileSystem):\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m     resolved_path = \u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresolve_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    487\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved_path.repo_id, resolved_path.revision\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fs, HTTPFileSystem) \u001b[38;5;129;01mand\u001b[39;00m data_file.startswith(config.HF_ENDPOINT):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/huggingface_hub/hf_file_system.py:198\u001b[39m, in \u001b[36mHfFileSystem.resolve_path\u001b[39m\u001b[34m(self, path, revision)\u001b[39m\n\u001b[32m    196\u001b[39m     path_in_repo = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    197\u001b[39m revision = _align_revision_in_path_with_revision(unquote(revision_in_path), revision)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m repo_and_revision_exist, err = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_repo_and_revision_exist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m repo_and_revision_exist:\n\u001b[32m    200\u001b[39m     _raise_file_not_found(path, err)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/huggingface_hub/hf_file_system.py:125\u001b[39m, in \u001b[36mHfFileSystem._repo_and_revision_exist\u001b[39m\u001b[34m(self, repo_type, repo_id, revision)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (repo_type, repo_id, revision) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._repo_and_revision_exists_cache:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepo_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHF_HUB_ETAG_TIMEOUT\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (RepositoryNotFoundError, HFValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    129\u001b[39m         \u001b[38;5;28mself\u001b[39m._repo_and_revision_exists_cache[(repo_type, repo_id, revision)] = \u001b[38;5;28;01mFalse\u001b[39;00m, e\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py:2734\u001b[39m, in \u001b[36mHfApi.repo_info\u001b[39m\u001b[34m(self, repo_id, revision, repo_type, timeout, files_metadata, expand, token)\u001b[39m\n\u001b[32m   2732\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2733\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsupported repo type.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2734\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2735\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2736\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2737\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2738\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2739\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2740\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiles_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiles_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2741\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py:2591\u001b[39m, in \u001b[36mHfApi.dataset_info\u001b[39m\u001b[34m(self, repo_id, revision, timeout, files_metadata, expand, token)\u001b[39m\n\u001b[32m   2588\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m expand:\n\u001b[32m   2589\u001b[39m     params[\u001b[33m\"\u001b[39m\u001b[33mexpand\u001b[39m\u001b[33m\"\u001b[39m] = expand\n\u001b[32m-> \u001b[39m\u001b[32m2591\u001b[39m r = \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2592\u001b[39m hf_raise_for_status(r)\n\u001b[32m   2593\u001b[39m data = r.json()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/requests/sessions.py:602\u001b[39m, in \u001b[36mSession.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    595\u001b[39m \n\u001b[32m    596\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:96\u001b[39m, in \u001b[36mUniqueRequestIdAdapter.send\u001b[39m\u001b[34m(self, request, *args, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     98\u001b[39m     request_id = request.headers.get(X_AMZN_TRACE_ID)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/projects/portfolio/project/ml_apps/hf_agents_course/.venv/lib/python3.12/site-packages/requests/adapters.py:713\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    711\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request=request)\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n\u001b[32m    715\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidHeader(e, request=request)\n",
            "\u001b[31mReadTimeout\u001b[39m: (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 8baa6922-a6ce-4330-a752-bc907107c544)')"
          ]
        }
      ],
      "source": [
        "eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcwcKUw4NwbP"
      },
      "source": [
        "Before running the test let's make the agent less verbose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UijuG07NwbP"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "agent.logger.setLevel(logging.WARNING) # Let's reduce the agent's verbosity level\n",
        "\n",
        "eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYuD7mzCNwbP"
      },
      "outputs": [],
      "source": [
        "outputs_agentic_rag = []\n",
        "\n",
        "for example in tqdm(eval_dataset):\n",
        "    question = example[\"question\"]\n",
        "\n",
        "    enhanced_question = f\"\"\"Using the information contained in your knowledge base, which you can access with the 'retriever' tool,\n",
        "give a comprehensive answer to the question below.\n",
        "Respond only to the question asked, response should be concise and relevant to the question.\n",
        "If you cannot find information, do not give up and try calling your retriever again with different arguments!\n",
        "Make sure to have covered the question completely by calling the retriever tool several times with semantically different queries.\n",
        "Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\n",
        "\n",
        "Question:\n",
        "{question}\"\"\"\n",
        "    answer = agent.run(enhanced_question)\n",
        "    print(\"=======================================================\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "    print(f'True answer: {example[\"answer\"]}')\n",
        "\n",
        "    results_agentic = {\n",
        "        \"question\": question,\n",
        "        \"true_answer\": example[\"answer\"],\n",
        "        \"source_doc\": example[\"source_doc\"],\n",
        "        \"generated_answer\": answer,\n",
        "    }\n",
        "    outputs_agentic_rag.append(results_agentic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xub7KUatNwbP"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "reader_llm = InferenceClient(\"Qwen/Qwen2.5-72B-Instruct\")\n",
        "\n",
        "outputs_standard_rag = []\n",
        "\n",
        "for example in tqdm(eval_dataset):\n",
        "    question = example[\"question\"]\n",
        "    context = retriever_tool(question)\n",
        "\n",
        "    prompt = f\"\"\"Given the question and supporting documents below, give a comprehensive answer to the question.\n",
        "Respond only to the question asked, response should be concise and relevant to the question.\n",
        "Provide the number of the source document when relevant.\n",
        "If you cannot find information, do not give up and try calling your retriever again with different arguments!\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "{context}\n",
        "\"\"\"\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    answer = reader_llm.chat_completion(messages).choices[0].message.content\n",
        "\n",
        "    print(\"=======================================================\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "    print(f'True answer: {example[\"answer\"]}')\n",
        "\n",
        "    results_agentic = {\n",
        "        \"question\": question,\n",
        "        \"true_answer\": example[\"answer\"],\n",
        "        \"source_doc\": example[\"source_doc\"],\n",
        "        \"generated_answer\": answer,\n",
        "    }\n",
        "    outputs_standard_rag.append(results_agentic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRL7b3WINwbQ"
      },
      "source": [
        "The evaluation prompt follows some of the best principles shown in [our llm_judge cookbook](llm_judge): it follows a small integer Likert scale, has clear criteria, and a description for each score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-V-oCKyNwbS"
      },
      "outputs": [],
      "source": [
        "EVALUATION_PROMPT = \"\"\"You are a fair evaluator language model.\n",
        "\n",
        "You will be given an instruction, a response to evaluate, a reference answer that gets a score of 3, and a score rubric representing a evaluation criteria are given.\n",
        "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
        "2. After writing a feedback, write a score that is an integer between 1 and 3. You should refer to the score rubric.\n",
        "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 3}}\\\"\n",
        "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
        "5. Do not score conciseness: a correct answer that covers the question should receive max score, even if it contains additional useless information.\n",
        "\n",
        "The instruction to evaluate:\n",
        "{instruction}\n",
        "\n",
        "Response to evaluate:\n",
        "{response}\n",
        "\n",
        "Reference Answer (Score 3):\n",
        "{reference_answer}\n",
        "\n",
        "Score Rubrics:\n",
        "[Is the response complete, accurate, and factual based on the reference answer?]\n",
        "Score 1: The response is completely incomplete, inaccurate, and/or not factual.\n",
        "Score 2: The response is somewhat complete, accurate, and/or factual.\n",
        "Score 3: The response is completely complete, accurate, and/or factual.\n",
        "\n",
        "Feedback:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFD0K_a_NwbS"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "evaluation_client = InferenceClient(\"meta-llama/Llama-3.1-70B-Instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpU8NlEaNwbS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "results = {}\n",
        "for system_type, outputs in [\n",
        "    (\"agentic\", outputs_agentic_rag),\n",
        "    (\"standard\", outputs_standard_rag),\n",
        "]:\n",
        "    for experiment in tqdm(outputs):\n",
        "        eval_prompt = EVALUATION_PROMPT.format(\n",
        "            instruction=experiment[\"question\"],\n",
        "            response=experiment[\"generated_answer\"],\n",
        "            reference_answer=experiment[\"true_answer\"],\n",
        "        )\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a fair evaluator language model.\"},\n",
        "            {\"role\": \"user\", \"content\": eval_prompt},\n",
        "        ]\n",
        "\n",
        "        eval_result = evaluation_client.text_generation(\n",
        "            eval_prompt, max_new_tokens=1000\n",
        "        )\n",
        "        try:\n",
        "            feedback, score = [item.strip() for item in eval_result.split(\"[RESULT]\")]\n",
        "            experiment[\"eval_score_LLM_judge\"] = score\n",
        "            experiment[\"eval_feedback_LLM_judge\"] = feedback\n",
        "        except:\n",
        "            print(f\"Parsing failed - output was: {eval_result}\")\n",
        "\n",
        "    results[system_type] = pd.DataFrame.from_dict(outputs)\n",
        "    results[system_type] = results[system_type].loc[~results[system_type][\"generated_answer\"].str.contains(\"Error\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtDxXxjQNwbS",
        "outputId": "086175f5-4767-4cac-88a4-1fb058c045e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average score for agentic RAG: 86.9%\n",
            "Average score for standard RAG: 73.1%\n"
          ]
        }
      ],
      "source": [
        "DEFAULT_SCORE = 2 # Give average score whenever scoring fails\n",
        "def fill_score(x):\n",
        "    try:\n",
        "        return int(x)\n",
        "    except:\n",
        "        return DEFAULT_SCORE\n",
        "\n",
        "for system_type, outputs in [\n",
        "    (\"agentic\", outputs_agentic_rag),\n",
        "    (\"standard\", outputs_standard_rag),\n",
        "]:\n",
        "\n",
        "    results[system_type][\"eval_score_LLM_judge_int\"] = (\n",
        "        results[system_type][\"eval_score_LLM_judge\"].fillna(DEFAULT_SCORE).apply(fill_score)\n",
        "    )\n",
        "    results[system_type][\"eval_score_LLM_judge_int\"] = (results[system_type][\"eval_score_LLM_judge_int\"] - 1) / 2\n",
        "\n",
        "    print(\n",
        "        f\"Average score for {system_type} RAG: {results[system_type]['eval_score_LLM_judge_int'].mean()*100:.1f}%\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZzJL_4_NwbT"
      },
      "source": [
        "**Let us recap: the Agent setup improves scores by 14% compared to a standard RAG!** (from 73.1% to 86.9%)\n",
        "\n",
        "This is a great improvement, with a very simple setup ğŸš€\n",
        "\n",
        "(For a baseline, using Llama-3-70B without the knowledge base got 36%)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
